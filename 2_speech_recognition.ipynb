{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ! pip install --user google.cloud\n",
    "# ! pip install --user google.cloud.speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from google.cloud import speech\n",
    "from google.cloud.speech import enums\n",
    "from google.cloud.speech import types\n",
    "from multiprocessing.dummy import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"list_of_roads.txt\", \"rb\") as fp:\n",
    "    list_of_roads = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition & Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can attempt to extract location we need to transcribe our audio. Here we used [Google Cloud Speech-to-Text](https://cloud.google.com/speech-to-text/) on our speech samples. Google's is a highly reputed and easy-to-use transcription API. It is also about as \"black box\" as it comes. The Speech-to-Text is a completely proprietary neural network that we only know by the output it gives us and the inputs it affords us. \n",
    "\n",
    "Notably, we are able to provide a vocabulary list. This list is used as what are known as \"contextual embeddings\". Contextual embeddings essentially skew the weight of the terms provided favorably over those that the speech recognition client would otherwise predict as the word spoken. Given we know the names of all roads we expect to hear, we use our scraped list as embeddings, thereby giving the otherwise \"universal\" speech recognition client a context. The embeddings do not deactivate the training of the neural network however, so we will see the impact that our embeddings have on the confidence score that the Google Cloud Speech-to-Text also provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(12) \n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./api_key.json\"   \n",
    "client = speech.SpeechClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Contextual Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define two functions, one which retrieves files, and the other which transcribes files and expects `speech_contexts` to be provided. We then run the two functions using `pool` multiprocessing which increases the speed of the call. Our speech samples are transcribed and returned as a pandas dataframe. Finally we take the average confidence score that Google gives in order to score this run with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcripts</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stetson</td>\n",
       "      <td>0.542913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Isabelle never to mobile crisis or crisis in G...</td>\n",
       "      <td>0.741057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66 is on it</td>\n",
       "      <td>0.770346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>silver foxes</td>\n",
       "      <td>0.444745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pratt for theis wonderful</td>\n",
       "      <td>0.744394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         transcripts  confidence\n",
       "0                                            Stetson    0.542913\n",
       "1  Isabelle never to mobile crisis or crisis in G...    0.741057\n",
       "2                                        66 is on it    0.770346\n",
       "3                                       silver foxes    0.444745\n",
       "4                          Pratt for theis wonderful    0.744394"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_desired_files(directory_name):\n",
    "    desired_files = []\n",
    "    for filename in os.listdir(directory_name):\n",
    "        if (os.path.getsize(directory_name + '/' + filename) < 2_600_000) & (filename.endswith('.wav')):\n",
    "            desired_files.append(directory_name + '/' + filename)\n",
    "    return desired_files\n",
    "\n",
    "def transcribe_embeddings(file_name):\n",
    "    transcript = ''\n",
    "    conf = 0\n",
    "    # Loads the audio into memory\n",
    "    with io.open(file_name, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    #speech_to_text\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        language_code='en-US',\n",
    "        model=\"video\",\n",
    "        speech_contexts = [{\n",
    "                        \"phrases\": np.random.choice(list_of_roads, 5000)\n",
    "                         }]\n",
    "    )\n",
    "\n",
    "    # Detects speech in the audio file\n",
    "    response = client.recognize(config, audio)\n",
    "\n",
    "    for result in response.results:\n",
    "        transcript = result.alternatives[0].transcript\n",
    "        conf = result.alternatives[0].confidence\n",
    "\n",
    "    time.sleep(1)\n",
    "    \n",
    "    return transcript, conf, file_name\n",
    "\n",
    "pool = Pool(12) \n",
    "\n",
    "list_of_transcripts_context = pool.map(transcribe_embeddings, get_desired_files('./samples_1'))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "transcripts_context = [a[0] for a in list_of_transcripts_context if a[0] != '']\n",
    "confs_context = [a[1] for a in list_of_transcripts_context if a[0] != '']\n",
    "names_context = [a[2] for a in list_of_transcripts_context if a[0] != '']\n",
    "\n",
    "data_context = {'transcripts': transcripts_context, \n",
    "                'confidence': confs_context}\n",
    "df_context = pd.DataFrame(data_context)\n",
    "df_context.to_csv('./datasets/radio_embedded.csv')\n",
    "df_context.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6713830406546055"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df_context['confidence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Contextual Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, we define one more function. This time we are doing transcription without including context. We again score it by taking the average of the confidence scores given by Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(file_name):\n",
    "    transcript = ''\n",
    "    conf = 0\n",
    "    # Loads the audio into memory\n",
    "    with io.open(file_name, 'rb') as audio_file:\n",
    "        content = audio_file.read()\n",
    "        audio = types.RecognitionAudio(content=content)\n",
    "\n",
    "    #speech_to_text\n",
    "    config = types.RecognitionConfig(\n",
    "        encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        language_code='en-US',\n",
    "        model=\"video\"\n",
    "    )\n",
    "\n",
    "    # Detects speech in the audio file\n",
    "    response = client.recognize(config, audio)\n",
    "\n",
    "    for result in response.results:\n",
    "        transcript = result.alternatives[0].transcript\n",
    "        conf = result.alternatives[0].confidence\n",
    "\n",
    "    time.sleep(1)\n",
    "    \n",
    "    return transcript, conf, file_name\n",
    "\n",
    "pool = Pool(12) \n",
    "\n",
    "list_of_transcripts = pool.map(transcribe, get_desired_files('./samples_1'))\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "transcripts = [a[0] for a in list_of_transcripts if a[0] != '']\n",
    "confs = [a[1] for a in list_of_transcripts if a[0] != '']\n",
    "names = [a[2] for a in list_of_transcripts if a[0] != '']\n",
    "\n",
    "data = {'transcripts': transcripts, \n",
    "        'confidence': confs}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('./datasets/radio_unembedded.csv')\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7757142536223881"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GAdsi]",
   "language": "python",
   "name": "conda-env-GAdsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
